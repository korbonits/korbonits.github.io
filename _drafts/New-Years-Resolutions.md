---
layout: post
title:  "New Years Resolutions 2018"
date:   2017-12-10
---

## Hold me accountable

This year, I propose the following set of New Years Resolutions. I am posting this publically so you can hold me accountable. I'll do a retrospective at the end of 2018.

Furthermore, some of these need refining, e.g., submitting a paper to NIPS or giving a talk. "Read 50 books" is not well-defined but is very easy to define. I usually only pick a next book once I've finished one.

- Read all papers in Microsoft Research's "Paper Legend: Artificial Intelligence Edition" card game
- Submit a paper to a NIPS workshop
- Give a talk at a conference
- Read 50 books (tracking with Goodreads)
- Learn how to race cars
- Enter a sailboat race
- Perform something creative (sing, act, play an instrument)
- Write more blog posts
- Get organized

## Paper Legend: Artifical Intelligence Edition

This resolution is inspired by my attendance at NIPS 2017. Microsoft was a sponsor and gave out these nerdy-looking card packs as swag.

Each card looks like this:

![Batch Norm](/assets/batch-norm.png)

Ioffe, Sergey, and Christian Szegedy. "Batch normalization: Accelerating deep network training by reducing internal covariate shift." In International Conference on Machine Learning, pp. 448-456. 2015.

He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. "Deep residual learning for image recognition." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. 2016.

Dahl, George E., Dong Yu, Li Deng, and Alex Acero. "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition." IEEE Transactions on audio, speech, and language processing 20, no. 1 (2012): 30-42.

Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "Sequence to sequence learning with neural networks." In Advances in neural information processing systems, pp. 3104-3112. 2014.

Bishop, Chris M. "Training with noise is equivalent to Tikhonov regularization." Training 7, no. 1 (2008).

Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9, no. 8 (1997): 1735-1780.

Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller. "Nonlinear component analysis as a kernel eigenvalue problem." Neural computation 10, no. 5 (1998): 1299-1319.

Bell, Anthony J., and Terrence J. Sejnowski. "An information-maximization approach to blind separation and blind deconvolution." Neural computation 7, no. 6 (1995): 1129-1159.

Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. "Learning long-term dependencies with gradient descent is difficult." IEEE transactions on neural networks 5, no. 2 (1994): 157-166.

Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. "Dropout: a simple way to prevent neural networks from overfitting." Journal of machine learning research 15, no. 1 (2014): 1929-1958.

Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440. 2015.

Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. "Practical bayesian optimization of machine learning algorithms." In Advances in neural information processing systems, pp. 2951-2959. 2012.

Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves et al. "Human-level control through deep reinforcement learning." Nature 518, no. 7540 (2015): 529-533.

Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." Journal of machine Learning research 3, no. Jan (2003): 993-1022.

Kingma, Diederik P., and Max Welling. "Auto-encoding variational bayes." arXiv preprint arXiv:1312.6114 (2013).

Dechter, Rina, Itay Meiri, and Judea Pearl. "Temporal constraint networks." Artificial intelligence 49, no. 1-3 (1991): 61-95.

LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. "Gradient-based learning applied to document recognition." Proceedings of the IEEE 86, no. 11 (1998): 2278-2324.

Sutton, Richard S. "Learning to predict by the methods of temporal differences." Machine learning 3, no. 1 (1988): 9-44.

Boser, Bernhard E., Isabelle M. Guyon, and Vladimir N. Vapnik. "A training algorithm for optimal margin classifiers." In Proceedings of the fifth annual workshop on Computational learning theory, pp. 144-152. ACM, 1992.

Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." In Advances in neural information processing systems, pp. 1097-1105. 2012.

Hinton, Geoffrey E., and Drew Van Camp. "Keeping the neural networks simple by minimizing the description length of the weights." In Proceedings of the sixth annual conference on Computational learning theory, pp. 5-13. ACM, 1993.

Dayan, Peter, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel. "The helmholtz machine." Neural computation 7, no. 5 (1995): 889-904.

Murphy, Kevin P., Yair Weiss, and Michael I. Jordan. "Loopy belief propagation for approximate inference: An empirical study." In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pp. 467-475. Morgan Kaufmann 
blishers Inc., 1999.

Neal, Radford M., and Geoffrey E. Hinton. "A view of the EM algorithm that justifies incremental, sparse, and other variants." In Learning in graphical models, pp. 355-368. Springer Netherlands, 1998.

Roweis, Sam, and Zoubin Ghahramani. "A unifying review of linear Gaussian models." Neural computation 11, no. 2 (1999): 305-345.

Minka, Tom. Divergence measures and message passing. Technical report, Microsoft Research, 2005.

Sutton, Richard S., David A. McAllester, Satinder P. Singh, and Yishay Mansour. "Policy gradient methods for reinforcement learning with function approximation." In Advances in neural information processing systems, pp. 1057-1063. 2000.

Cortes, Corinna, and Vladimir Vapnik. "Support-vector networks." Machine learning 20, no. 3 (1995): 273-297.

Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional networks." In European conference on computer vision, pp. 818-833. Springer, Cham, 2014.

MacKay, David JC. "A practical Bayesian framework for backpropagation networks." Neural computation 4, no. 3 (1992): 448-472.

Fei-Fei, Li, and Pietro Perona. "A bayesian hierarchical model for learning natural scene categories." In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2, pp. 524-531. IEEE, 2005.

Tenenbaum, Joshua B., Vin De Silva, and John C. Langford. "A global geometric framework for nonlinear dimensionality reduction." science 290, no. 5500 (2000): 2319-2323.

Taskar, Ben, Carlos Guestrin, and Daphne Koller. "Max-margin Markov networks." In Advances in neural information processing systems, pp. 25-32. 2004.

Jaakkola, Tommi, and David Haussler. "Exploiting generative models in discriminative classifiers." In Advances in neural information processing systems, pp. 487-493. 1999. Harvard	

Watkins, Christopher JCH, and Peter Dayan. "Q-learning." Machine learning 8, no. 3-4 (1992): 279-292.
